import pickle
import math
import torch
from torch import nn, einsum
import random
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from tdc.single_pred import ADME
from rdkit import Chem
from rdkit.Chem import Recap, BRICS, rdFMCS
from rdkit.Chem.Scaffolds import MurckoScaffold
from rdkit.Chem.rdchem import RWMol
from rdkit.Chem import Draw
from rdkit.Chem import AllChem
import networkx as nx
import numpy as np
import pandas as pd
import torch
import networkx as nx
import matplotlib.pyplot as plt
import umap
import seaborn as sns
from itertools import chain
import copy
import umap

from torch.optim import Adam
from torch.utils.data import DataLoader, TensorDataset, random_split

import matplotlib.pyplot as plt
import networkx as nx
from rdkit import Chem
from rdkit.Chem import Draw

import copy
from collections import defaultdict
import re
from collections import defaultdict
from itertools import zip_longest
from transformers import AutoTokenizer, AutoModelForMaskedLM
import torch
from tqdm import tqdm  # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, classification_report, roc_curve
import lightgbm as lgb
from sklearn.preprocessing import StandardScaler

from sklearn.metrics.pairwise import pairwise_distances
from rdkit import Chem
from rdkit.Chem import Draw, AllChem
from rdkit import DataStructs
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.gridspec as gridspec
import matplotlib.patches as patches

import argparse
from pathlib import Path

import sys
sys.path.append("/data1/project/yeeun/MAGNET_final/pretraining/")
from graph_transformer import GraphTransformerModel

sys.path.append("/data1/project/yeeun/MAGNET_final/preprocessing/")
from preprocess_utils import smiles_to_vector


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


def collate_fn(batch):
    """
    DataLoaderì—ì„œ ì‚¬ìš©ë˜ëŠ” collate í•¨ìˆ˜.
    edgeì™€ adjacencyê°€ ë™ì¼í•œ í¬ê¸°ì¸ [num_nodes, num_nodes] í˜•ì‹ì¸ ê²½ìš°ì— ë§ê²Œ ë°ì´í„°ë¥¼ íŒ¨ë”©í•©ë‹ˆë‹¤.
    """
    # ì²« ë²ˆì§¸ ë·°ë§Œ ì‚¬ìš© (view_1ë§Œ ìœ ì§€)
    view_1_nodes = [item.x for item in batch]
    view_1_edges = [item.edge for item in batch]
    view_1_adjacency = [item.adjacency for item in batch]

    def pad_graphs(nodes_batch, edges_batch, adjacency_batch):
        # ë…¸ë“œ ë° edge/adjacencyì˜ ìµœëŒ€ í¬ê¸°ë¥¼ ë”°ë¡œ í™•ì¸
        max_node_size = max(node.size(0) for node in nodes_batch)
        max_edge_size = max(edge.size(0) for edge in edges_batch)
        max_adj_size = max(adj.size(0) for adj in adjacency_batch)

        # ëª¨ë“  paddingì€ ì´ë“¤ ì¤‘ ê°€ì¥ í° ê±¸ë¡œ ë§ì¶˜ë‹¤
        max_size = max(max_node_size, max_edge_size, max_adj_size)
        feature_dim = nodes_batch[0].size(1)

        # ë…¸ë“œ íŒ¨ë”©
        padded_nodes = torch.zeros(len(nodes_batch), max_size, feature_dim)
        for i, node in enumerate(nodes_batch):
            if node.size(0) == 0: print(f"Warning: Graph {i} has no nodes!")
            padded_nodes[i, :node.size(0), :] = node

        # ì—£ì§€ ì •ë³´ íŒ¨ë”©
        padded_edges = torch.zeros(len(edges_batch), max_size, max_size, 1)  # Ensure 4D
        for i, edge in enumerate(edges_batch):
            if edge.dim() == 2:  # If edges are 2D, add a singleton dimension
                edge = edge.unsqueeze(-1)  # (num_nodes, num_nodes) -> (num_nodes, num_nodes, 1)
            edge_size = edge.size(0)
            padded_edges[i, :edge_size, :edge_size, :] = edge

        # ì¸ì ‘ í–‰ë ¬ íŒ¨ë”©
        padded_adjacency = torch.zeros(len(adjacency_batch), max_size, max_size)
        for i, adjacency in enumerate(adjacency_batch):
            adj_size = adjacency.size(0)
            padded_adjacency[i, :adj_size, :adj_size] = adjacency

        return padded_nodes, padded_edges, padded_adjacency

    # ì²« ë²ˆì§¸ ë·° íŒ¨ë”©ë§Œ ë°˜í™˜
    view_1_padded = pad_graphs(view_1_nodes, view_1_edges, view_1_adjacency)

    return view_1_padded  # âœ… ë·° í•˜ë‚˜ë§Œ ë°˜í™˜

import torch

def preprocess_batch(batch):
    """
    ë°°ì¹˜ ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
    `collate_fn()`ì—ì„œ ë°˜í™˜í•œ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    # `batch`ì—ì„œ ë…¸ë“œ, ì—£ì§€, ì¸ì ‘ í–‰ë ¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    node_features, edge_matrices, adjacency_matrices = batch

    # GPUë¡œ ì´ë™ ë° ë°ì´í„° íƒ€ì… ë³€í™˜
    node_features = node_features.float().to(device)  # [batch_size, max_nodes, feature_dim]
    edge_matrices = edge_matrices.to(device)  # [batch_size, max_nodes, max_nodes, 1]
    adjacency_matrices = adjacency_matrices.to(device)  # [batch_size, max_nodes, max_nodes]

    return node_features, edge_matrices, adjacency_matrices

def extract_graph_embedding(model, loader, pooling="mean"):
    """
    ëª¨ë¸ê³¼ ë°ì´í„° ë¡œë”ë¥¼ ì‚¬ìš©í•´ ê·¸ë˜í”„ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        model (nn.Module): í•™ìŠµëœ Graph Transformer ëª¨ë¸
        loader (DataLoader): ë°ì´í„° ë¡œë”
        pooling (str): "mean", "max", "sum" ì¤‘ ì„ íƒ
        
    Returns:
        graph_embeddings (torch.Tensor): ê·¸ë˜í”„ ì„ë² ë”© (batch_size, embedding_dim)
    """
    model.eval()
    graph_embeddings = []
    
    with torch.no_grad():
        for batch in loader:
            # ë°°ì¹˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            node_features, edge_matrices, adjacency_matrices = preprocess_batch(batch)
            
            # ë…¸ë“œ ì„ë² ë”© ìƒì„±
            node_embeddings = model(node_features, edge_matrices, adjacency_matrices)
            
            # ê·¸ë˜í”„ ì„ë² ë”© ìƒì„±
            if pooling == "mean":
                # graph_embedding = node_embeddings.mean(dim=1)  # [batch_size, embedding_dim]
                mask = adjacency_matrices.sum(dim=-1) > 0  # [batch_size, max_nodes]
                mask = mask.float().unsqueeze(-1)  # [batch_size, max_nodes, 1]
                
                # ğŸŸ¢ NaNì„ 0ìœ¼ë¡œ ëŒ€ì²´
                node_embeddings = torch.nan_to_num(node_embeddings, nan=0.0)
                
                masked_embeddings = node_embeddings * mask
                denom = mask.sum(dim=1).clamp(min=1)  # ì‹¤ì œ ìœ íš¨ ë…¸ë“œ ê°œìˆ˜
                graph_embedding = masked_embeddings.sum(dim=1) / denom
            elif pooling == "max":
                graph_embeddinag, _ = node_embeddings.max(dim=1)  # [batch_size, embedding_dim]
            elif pooling == "sum":
                graph_embedding = node_embeddings.sum(dim=1)  # [batch_size, embedding_dim]
            else:
                raise ValueError(f"Unsupported pooling method: {pooling}")
            
            graph_embeddings.append(graph_embedding)
    
    # ë°°ì¹˜ë³„ ê·¸ë˜í”„ ì„ë² ë”©ì„ í•˜ë‚˜ì˜ í…ì„œë¡œ ë³‘í•©
    graph_embeddings = torch.cat(graph_embeddings, dim=0)
    return graph_embeddings

def generate_combined_embeddings(df, graph_embeddings, device="cuda"):
    """
    ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ChemBERTa ì„ë² ë”©ê³¼ ê·¸ë˜í”„ ì„ë² ë”©ì„ concatí•˜ëŠ” í•¨ìˆ˜.

    Parameters:
    - df: SMILES ì—´ì„ í¬í•¨í•˜ëŠ” pandas DataFrame
    - graph_embeddings: ê¸°ì¡´ Graph ê¸°ë°˜ ì„ë² ë”© (torch.Tensor)
    - device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("cuda" or "cpu")

    Returns:
    - combined_embeddings: torch.Tensor, shape = (N, graph_dim + chemberta_dim)
    """
    # SMILES ë¦¬ìŠ¤íŠ¸
    smiles_list = list(df["smiles"])
    
    # ChemBERTa ì„ë² ë”©
    chemberta_embeddings = smiles_to_vector(smiles_list)  # (N, 768)
    chemberta_embeddings = chemberta_embeddings.to(device)
    
    # ê¸°ì¡´ ê·¸ë˜í”„ ì„ë² ë”©ë„ ë””ë°”ì´ìŠ¤ ì¼ì¹˜
    graph_embeddings = graph_embeddings.to(device)

    # ì„ë² ë”© ê²°í•©
    combined_embeddings = torch.cat([graph_embeddings, chemberta_embeddings], dim=1)

    return combined_embeddings


def main():
    
    parser = argparse.ArgumentParser(description="Run finetuning")
    parser.add_argument("input_file", help="Path to the input pickle file (contains 'all_graphs' and 'filtered_data').")
    parser.add_argument("--pt_file", required=True, help="Path to the trained model weights (.pt file).")
    args = parser.parse_args()

    input_file = args.input_file
    pt_file = args.pt_file
    
    if not Path(input_file).exists():
        print(f"âŒ Error: Input file does not exist: {input_file}")
        return

    # ğŸ”½ í”¼í´ íŒŒì¼ ë¡œë”©
    with open(input_file, "rb") as f:
        data = pickle.load(f)
        all_graphs = data["all_graphs"]
        filtered_data = data["filtered_data"]
        

    # ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    GraphTransformer_model = GraphTransformerModel(node_dim=768, edge_dim=1, num_blocks=4, num_heads=8, last_average=True, model_dim=256).to(device)

    # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    GraphTransformer_model.load_state_dict(torch.load(pt_file, map_location=device))

    GraphTransformer_model.eval()
    print("Model loaded successfully!")

    from torch_geometric.loader import DataLoader  

    # ë°ì´í„° ë¡œë”ë¥¼ torch_geometric.loader.DataLoaderë¡œ ë³€ê²½ 
    data_loader = torch.utils.data.DataLoader(all_graphs, batch_size=32, shuffle=True, collate_fn=collate_fn) 
    embeddings = extract_graph_embedding(GraphTransformer_model, data_loader, pooling="mean")
    print(f"dataset Graph embeddings shape: {embeddings.shape}")  # [num_graphs, embedding_dim]
    
    final_embeddings = generate_combined_embeddings(filtered_data, embeddings)
    
    
    output_dir = Path("finetuning_embedding")
    output_dir.mkdir(parents=True, exist_ok=True) 

    dataset_name = Path(input_file).stem
    output_filename = output_dir / f"embedding_{dataset_name}.pkl"

    combined_data = {
        "final_embeddings": final_embeddings,
    }

    print(len(final_embeddings))
    
    with open(output_filename, "wb") as f:
        pickle.dump(combined_data, f)
    
    

if __name__ == "__main__":
    main()

    
